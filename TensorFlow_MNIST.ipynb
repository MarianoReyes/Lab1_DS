{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Neuronal Profunda (DNN) para clasificación MNIST\n",
    "\n",
    "Aplicaremos todos nuestros conocimientos para crear una DNN.  El problema que vamos a trabajar se conoce como el \"Hola Mundo\" del aprendizaje profundo porque para la mayoría de estudiantes este es el primer algoritmo de aprendizaje profundo que ven. \n",
    "\n",
    "El conjunto de datos se llama MNIST y se refiere al reconocimiento de dígitos escritos a mano.  Pueden encontrar más información en el sitio web de Yann LeCun (Director of AI Research, Facebook).  El es uno de los pioneros de todo este tema, así como de otras metodologías más complejas como las Redes Neurales Convolucionales (CNN) que se utilizan hoy día.\n",
    "\n",
    "El conjunto de datos tiene 70,000 imágenes (28x28 pixels) de dígitos escritos a mano (1 dígito por imagen).\n",
    "\n",
    "La meta es escribir un algoritmo que detecta qué dígito ha sido escrito.  Como solo hay 10 dígitos (0 al 9), este es un problema de clasificación con 10 clases.\n",
    "\n",
    "Nuestra meta será construir una RN con 2 capas escondidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar los paquetes relevantes\n",
    "\n",
    "TensorFlow incluye un proveedor de datos de MNIST que utilizaremos acá.  Viene con el módulo **\"tensorflow-datasets\"** por lo que si no lo ha instalado aún, debe hacerlo:\n",
    "\n",
    "pip install tensorflow-datasets\n",
    "\n",
    "ó\n",
    "\n",
    "conda install tensorflow-datasets\n",
    "\n",
    "Estos conjuntos de datos se almacenarán en su directorio C:\\Users\\usuario\\tensorflow_datasets|...\n",
    "\n",
    "La primera vez que baje un conjunto de datos, se almacenará en la carpeta respectiva.  Cada vez subsiguiente, automáticamente cargará la copia en su computadora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_datasets\n",
      "  Downloading tensorflow_datasets-4.9.2-py3-none-any.whl (5.4 MB)\n",
      "     ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/5.4 MB 2.6 MB/s eta 0:00:03\n",
      "     - -------------------------------------- 0.2/5.4 MB 1.7 MB/s eta 0:00:04\n",
      "     -- ------------------------------------- 0.3/5.4 MB 1.7 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.4/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "     --- ------------------------------------ 0.5/5.4 MB 1.8 MB/s eta 0:00:03\n",
      "     ---- ----------------------------------- 0.6/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "     ----- ---------------------------------- 0.8/5.4 MB 1.8 MB/s eta 0:00:03\n",
      "     ------ --------------------------------- 0.9/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "     ------- -------------------------------- 1.0/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 1.1/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "     -------- ------------------------------- 1.2/5.4 MB 1.8 MB/s eta 0:00:03\n",
      "     --------- ------------------------------ 1.3/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 1.4/5.4 MB 1.8 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 1.6/5.4 MB 1.9 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 1.6/5.4 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.8/5.4 MB 1.9 MB/s eta 0:00:02\n",
      "     ------------- -------------------------- 1.8/5.4 MB 1.9 MB/s eta 0:00:02\n",
      "     -------------- ------------------------- 2.0/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 2.0/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     --------------- ------------------------ 2.2/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 2.3/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     ----------------- ---------------------- 2.4/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 2.4/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 2.6/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     ------------------- -------------------- 2.6/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 2.8/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 2.9/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 3.0/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 3.0/5.4 MB 1.7 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 3.2/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 3.3/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 3.4/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 3.5/5.4 MB 1.8 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 3.7/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 3.8/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 3.9/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 4.0/5.4 MB 1.8 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 4.2/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 4.4/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 4.4/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 4.6/5.4 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 4.9/5.4 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 5.0/5.4 MB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 5.2/5.4 MB 2.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.4/5.4 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.4/5.4 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  5.4/5.4 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 5.4/5.4 MB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: absl-py in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (1.4.0)\n",
      "Collecting array-record (from tensorflow_datasets)\n",
      "  Obtaining dependency information for array-record from https://files.pythonhosted.org/packages/4b/93/2a07d2f7317ea17ea9e17865d27175db13292ca8496aaeb6b9614aabe638/array_record-0.4.0-py39-none-any.whl.metadata\n",
      "  Downloading array_record-0.4.0-py39-none-any.whl.metadata (502 bytes)\n",
      "Requirement already satisfied: click in c:\\python39\\lib\\site-packages (from tensorflow_datasets) (8.1.0)\n",
      "Collecting dm-tree (from tensorflow_datasets)\n",
      "  Downloading dm_tree-0.1.8-cp39-cp39-win_amd64.whl (101 kB)\n",
      "     ---------------------------------------- 0.0/101.5 kB ? eta -:--:--\n",
      "     ----------------------------------- --- 92.2/101.5 kB 5.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 92.2/101.5 kB 5.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 92.2/101.5 kB 5.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 92.2/101.5 kB 5.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 92.2/101.5 kB 5.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 92.2/101.5 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ 101.5/101.5 kB 265.2 kB/s eta 0:00:00\n",
      "Collecting etils[enp,epath]>=0.9.0 (from tensorflow_datasets)\n",
      "  Obtaining dependency information for etils[enp,epath]>=0.9.0 from https://files.pythonhosted.org/packages/4a/6a/d58ec120f5e4babbf5001c144266ba623dcdae8e81dc6cdb422a98d0e0ce/etils-1.4.1-py3-none-any.whl.metadata\n",
      "  Downloading etils-1.4.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (1.23.3)\n",
      "Collecting promise (from tensorflow_datasets)\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf>=3.20 in c:\\python39\\lib\\site-packages (from tensorflow_datasets) (4.22.4)\n",
      "Requirement already satisfied: psutil in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (5.9.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (2.28.2)\n",
      "Collecting tensorflow-metadata (from tensorflow_datasets)\n",
      "  Downloading tensorflow_metadata-1.13.1-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: termcolor in c:\\python39\\lib\\site-packages (from tensorflow_datasets) (2.3.0)\n",
      "Requirement already satisfied: toml in c:\\python39\\lib\\site-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow_datasets) (4.64.1)\n",
      "Requirement already satisfied: wrapt in c:\\python39\\lib\\site-packages (from tensorflow_datasets) (1.14.1)\n",
      "Collecting importlib_resources (from etils[enp,epath]>=0.9.0->tensorflow_datasets)\n",
      "  Obtaining dependency information for importlib_resources from https://files.pythonhosted.org/packages/29/d1/bed03eca30aa05aaf6e0873de091f9385c48705c4a607c2dfe3edbe543e8/importlib_resources-6.0.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_resources-6.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing_extensions in c:\\python39\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (4.7.1)\n",
      "Requirement already satisfied: zipp in c:\\python39\\lib\\site-packages (from etils[enp,epath]>=0.9.0->tensorflow_datasets) (3.7.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python39\\lib\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.19.0->tensorflow_datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.19.0->tensorflow_datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jose\\appdata\\roaming\\python\\python39\\site-packages (from requests>=2.19.0->tensorflow_datasets) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\python39\\lib\\site-packages (from click->tensorflow_datasets) (0.4.4)\n",
      "Requirement already satisfied: six in c:\\python39\\lib\\site-packages (from promise->tensorflow_datasets) (1.16.0)\n",
      "Collecting googleapis-common-protos<2,>=1.52.0 (from tensorflow-metadata->tensorflow_datasets)\n",
      "  Obtaining dependency information for googleapis-common-protos<2,>=1.52.0 from https://files.pythonhosted.org/packages/a7/bc/416a1ffeba4dcd072bc10523dac9ed97f2e7fc4b760580e2bdbdc1e2afdd/googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading array_record-0.4.0-py39-none-any.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/3.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.4/3.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.5/3.0 MB 3.9 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.7/3.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.8/3.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.9/3.0 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.1/3.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 1.2/3.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.5/3.0 MB 3.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.6/3.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.8/3.0 MB 3.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.0/3.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 2.3/3.0 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 2.5/3.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 2.7/3.0 MB 3.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.9/3.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  3.0/3.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.0/3.0 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading googleapis_common_protos-1.60.0-py2.py3-none-any.whl (227 kB)\n",
      "   ---------------------------------------- 0.0/227.6 kB ? eta -:--:--\n",
      "   ---------------------------------- ----- 194.6/227.6 kB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  225.3/227.6 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  225.3/227.6 kB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 227.6/227.6 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading etils-1.4.1-py3-none-any.whl (135 kB)\n",
      "   ---------------------------------------- 0.0/135.8 kB ? eta -:--:--\n",
      "   ---------------------------------------  133.1/135.8 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  133.1/135.8 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 135.8/135.8 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading importlib_resources-6.0.0-py3-none-any.whl (31 kB)\n",
      "Building wheels for collected packages: promise\n",
      "  Building wheel for promise (setup.py): started\n",
      "  Building wheel for promise (setup.py): finished with status 'done'\n",
      "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21548 sha256=4673214d720a33d9b9ed243c8097dd7c5e7f519ad40cf6c40bf29346017376e3\n",
      "  Stored in directory: c:\\users\\jose\\appdata\\local\\pip\\cache\\wheels\\e1\\e8\\83\\ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "Successfully built promise\n",
      "Installing collected packages: dm-tree, promise, importlib_resources, googleapis-common-protos, etils, tensorflow-metadata, array-record, tensorflow_datasets\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\python39\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python39\\lib\\site-packages)\n",
      "  WARNING: Failed to write executable - trying to use .deleteme logic\n",
      "ERROR: Could not install packages due to an OSError: [WinError 2] El sistema no puede encontrar el archivo especificado: 'c:\\\\Python39\\\\Scripts\\\\tfds.exe' -> 'c:\\\\Python39\\\\Scripts\\\\tfds.exe.deleteme'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datos\n",
    "\n",
    "Esta sección es donde pre-procesaremos nuestros datos.\n",
    "\n",
    "**tfd.load** carga un conjunto de datos (o si es la primera vez, los baja y luego los carga).  En este caso estamos interesados en el conjunto de datos MNIST.  El único argumento obligatorio es el nombre del conjunto de datos.  Hay otros que pueden ser útiles, por ejemplo:\n",
    "\n",
    "**with_info = True** nos provee con una tupla que contiene información sobre la versión, features, número de observaciones (samples)\n",
    "\n",
    "**as_supervised = True** cargará el conjunto de datos en una estructura de 2 tuplas (entrada, meta).  Si se usa **False**, retorna un diccionario, obviamente preferimos tener de una vez nuestra entrada y meta separados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_mnist, info_mnist = tfds.load(name='mnist',\n",
    "                                    shuffle_files = False,\n",
    "                                    with_info=True, \n",
    "                                    as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se ha cargado el conjunto de datos, se pueden, fácilmente, extraer los conjuntos de entrenamiento y prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entreno_mnist, prueba_mnist = datos_mnist['train'], datos_mnist['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por default, TF2 tiene conjuntos de datos de entrenamiento y de prueba, pero no tiene un conjunto de validación, por lo que debemos dividirlo por nuestra cuenta\n",
    "\n",
    "Empezamos por definir el número de observaciones de validación, como un porcentaje de las observaciones de entrenamiento.  Aqui es donde también usamos **mnist_info** (no tenemos que contar las observaciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_validacion = 0.1 * info_mnist.splits['train'].num_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertimos este número a entero ya que un float puede causar problemas en el camino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_validacion = tf.cast(num_obs_validacion, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos una variable dedicada para el número de muestras de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_prueba = info_mnist.splits['test'].num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_obs_prueba = tf.cast(num_obs_prueba, tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalmente preferimos \"normalizar\" nuestros datos en alguna forma para que el resultado sea numéricamente más estable.  En este caso simplemente preferimos tener entradas entre 0 y 1, por lo que definimos una función, que reciba la imagen MNIST y su etiqueta, para hacerlo.\n",
    "\n",
    "Como los posibles valores de las entradas son entre 0 y 255 (256 posibles tonos de gris), al dividirlos por 255 obtenemos el resultado deseado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar(imagen, etiqueta):\n",
    "    imagen = tf.cast(imagen, tf.float32)\n",
    "    imagen /= 255.\n",
    "    return imagen, etiqueta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El método .map() nos permite aplicar una transormación \"customizada\" a un conjunto de datos.  Ya hemos decidido que obtendremos los datos de validación a partir de *mnist_train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datos_entrenamiento_y_validacion_normalizados = entreno_mnist.map(normalizar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, normalizaremos y convertiremos los datos de pruebas en tandas.  Los normalizamos para que tengan la misma magnitud que los datos de entrenamiento y validación.\n",
    "\n",
    "No hay necesidad de \"barajearlo\" ya que no estaremos entrenando con los datos de prueba.  Habra una sola tanda, igual al tamaño de los datos de prueba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_prueba = prueba_mnist.map(normalizar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si \"barajearemos\" los datos de entrenamiento y validación.\n",
    "\n",
    "El parámetro **TAMANIO_BUFFER** se utiliza para casos que tengan conjuntos de datos grandes.  En este caso no es posible \"barajear\" el conjunto completo de un solo porque no cabe en la memoria.  En vez, TF2 solo almacena los datos en memoria **TAMANIO_BUFFER** muestras a la vez, y los \"barajea\".\n",
    "\n",
    "si TAMANIO_BUFFER = 1 => no hay \"barajeo\"\n",
    "si TAMANIO_BUFFER >= número de muestras => el \"barajeo\" se hace uniformemente\n",
    "\n",
    "para un TAMANIO_BUFFER intermedio - se hace una optimización computacional para aproximar un \"barajeo\" uniforme.\n",
    "\n",
    "Afortunadamente, hay un método de \"barajeo\" disponible y solo necesitamos especificar el tamaño del buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAMANIO_BUFFER = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_entrenamiento_y_validacion_barajeados = datos_entrenamiento_y_validacion_normalizados.shuffle(TAMANIO_BUFFER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez se han \"normalizado\" y \"barajeado\" los datos, podemos proceder a extraer los datos de entrenamiento y de validación.\n",
    "\n",
    "Nuestros datos de validación serán el 10% del conjunto de entrenamiento, que ya se calculó utilizando el método **.take()**.\n",
    "\n",
    "Finalmente, creamos una tanda con un tamaño de tanda igual al total de muestras de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_validacion = datos_entrenamiento_y_validacion_barajeados.take(num_obs_validacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarmente, los datos de entrenamiento son todos los demás por lo que nos salteamos tantas observaciones como las hay en el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_entreno = datos_entrenamiento_y_validacion_barajeados.skip(num_obs_validacion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Establecemos el tamaño de las tandas.\n",
    "\n",
    "También podemos aprovechar el momento para separar los datos de entrenamiento y de prueba.\n",
    "\n",
    "Estos serán muy útiles cuando entrenemos, ya que podemos iterar sobre las diferentes tandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAMANIO_TANDA = 100\n",
    "\n",
    "datos_entreno = datos_entreno.batch(TAMANIO_TANDA)\n",
    "\n",
    "datos_validacion = datos_validacion.batch(num_obs_validacion)\n",
    "\n",
    "datos_prueba = datos_prueba.batch(num_obs_prueba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toma la siguiente tanda (es la única tanda) ya que, como configuramos **as_supervized = True**, obtuvimos una estructura de 2 tuplas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas_validacion, metas_validacion = next(iter(datos_validacion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delineamos el modelo\n",
    "\n",
    "Cuando pensamos sobre un algoritmo de aprenzaje profundo, casi siempre solo lo imaginamos.  Asi que esta vez, hagámoslo.  :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tamanio_entrada = 784\n",
    "tamanio_salida = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usaremos el mismo ancho para ambas capas escondidas.  No es una necesidad!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanio_capa_escondida = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definimos cómo se verá el modelo\n",
    "\n",
    "La primera capa (la de entrada):  cada observación es de 28x28x1 píxeles, por lo tanto es un tensor de rango 3.\n",
    "\n",
    "Como aún no hemos aprendido sobre CNNs, no sabemos como alimentar este tipo de entrada a nuestra red, por lo tanto hay que \"aplanar\" las imágenes.  Hay un método conveniente **Flatten** que toma nuestro tensor de 28x28x1 y lo convierte en  un vector (None), o (784,)...porque 28x28x1 = 784.  Esto nos permite crear una red de alimentación hacia adelante.\n",
    "\n",
    "    \n",
    "**tf.keras.layers.Dense** básicamente implementa:  output = activation(dot(entrada, peso) + sesgo).  Requiere varios argumentos, pero los más importantes para nosotros son el ancho de la capa escondida y la función de activación.\n",
    "\n",
    "La capa final no es diferente, solo nos aseguramos de activarla con **softmax**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo = tf.keras.Sequential([\n",
    "\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28, 1)), # capa entrada\n",
    "    \n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 1era capa escondida\n",
    "    tf.keras.layers.Dense(tamanio_capa_escondida, activation='relu'), # 2nda capa escondida\n",
    "\n",
    "    tf.keras.layers.Dense(tamanio_salida, activation='softmax') # capa salida\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleccionar el optimizador y la función de pérdida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Definimos el optimizador que nos gustaría utilizar, la función de pérdida, y las métricas que nos interesa obtener en cada interacción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento\n",
    "\n",
    "Acá es donde entrenamos el modelo que hemos construído\n",
    "\n",
    "Determinamos el número máximo de épocas.\n",
    "\n",
    "Ajustamos el modelo , especificando:\n",
    "\n",
    "* los datos de entrenamiento\n",
    "* el número total de épocas\n",
    "* y los datos de validación que creamos en el formato (entradas, metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERO_EPOCAS = 5\n",
    "\n",
    "modelo.fit(datos_entreno, \n",
    "          epochs = NUMERO_EPOCAS, \n",
    "          validation_data = (entradas_validacion, metas_validacion),\n",
    "          validation_steps = 10,\n",
    "          verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probar el modelo\n",
    "\n",
    "Como se discutió en clase, luego del entrenamiento (con los datos de entrenamiento), y la validación (con los datos de validación), probamos el potencial de predicción final de nuestro modelo con el conjunto de datos de prueba que el algoritmo NUNCA ha visto antes.\n",
    "\n",
    "Es muy importante reconocer que estar \"jugando\" con los hiperparámetros sobre-ajusta el conjunto de datos de validación.\n",
    "\n",
    "La prueba es la instancia absolutamente final. Nunca debe probarse el modelo antes de haber completamente ajustado el modelo.\n",
    "\n",
    "Si se ajusta el modelo después de hacer la prueba, se empezará a sobre-ajustar el conjunto de datos de prueba, que echaría \"por los suelos\" el propósito original del mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perdida_prueba, precision_prueba = modelo.evaluate(datos_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si se desea, se puede aplicar un formateo \"bonito\"\n",
    "print('Pérdida de prueba: {0:.2f}. Precisión de prueba: {1:.2f}%'.format(perdida_prueba, precision_prueba * 100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando el modelo inicial y los hiperparámetros dados en este notebook, la precisión de prueba final debe ser aproximadamente 97%.\n",
    "\n",
    "Cada vez que se ejecuta el código, se obtiene una precisión diferente debido a la \"barajeada\" de las tandas, los pesos se inicializan en forma diferente, etc.\n",
    "\n",
    "Finalmente, intencionalmente se ha llegado a una solución subóptima, para que pueda tener la oportunidad de mejorarla."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
